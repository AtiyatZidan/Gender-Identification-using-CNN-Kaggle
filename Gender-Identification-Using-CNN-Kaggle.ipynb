# ============================================================
# Project: Gender Identification Using Machine Learning
# ============================================================

import os
import math
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import classification_report, confusion_matrix

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import AdamW
from tensorflow.keras import layers, models
from tensorflow.keras.regularizers import l2

# 1. GPU Configuration and Memory Management
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print(f"Running on GPU: {gpus[0].name}")
    except RuntimeError as e:
        print(f"GPU Runtime Error: {e}")
else:
    print("No GPU detected. Switching to CPU.")

# 2. Dataset Path Configuration (Optimized for Kaggle environment)
train_dir = "/kaggle/input/gender-classification-data-set/Dataset/Train"
val_dir = "/kaggle/input/gender-classification-data-set/Dataset/Validation"
test_dir = "/kaggle/input/gender-classification-data-set/Dataset/Test"

# 3. Model Training Parameters
IMG_SIZE = (128, 128)
BATCH_SIZE = 128
LEARNING_RATE = 1e-4
EPOCHS = 50

# 4. Data Augmentation Pipelines
train_datagen = ImageDataGenerator(
    rescale=1.0 / 255.0,
    rotation_range=20,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    brightness_range=(0.9, 1.1)
)

val_test_datagen = ImageDataGenerator(rescale=1.0 / 255.0)

# Data Loading
train_generator = train_datagen.flow_from_directory(
    train_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='binary', shuffle=True
)

val_generator = val_test_datagen.flow_from_directory(
    val_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='binary', shuffle=False
)

test_generator = val_test_datagen.flow_from_directory(
    test_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='binary', shuffle=False
)

# 5. Class Imbalance Handling
classes = train_generator.classes
class_weights = compute_class_weight('balanced', classes=np.unique(classes), y=classes)
class_weights_dict = dict(enumerate(class_weights))
print(f"Calculated Class Weights: {class_weights_dict}")

# 6. Custom Performance Metrics (F1-Score Implementation)
class F1Score(tf.keras.metrics.Metric):
    def __init__(self, name='f1_score', threshold=0.5, **kwargs):
        super(F1Score, self).__init__(name=name, **kwargs)
        self.threshold = threshold
        self.tp = self.add_weight(name='tp', initializer='zeros')
        self.fp = self.add_weight(name='fp', initializer='zeros')
        self.fn = self.add_weight(name='fn', initializer='zeros')

    def update_state(self, y_true, y_pred, sample_weight=None):
        y_pred = tf.cast(y_pred > self.threshold, tf.float32)
        y_true = tf.cast(y_true, tf.float32)
        self.tp.assign_add(tf.reduce_sum(y_true * y_pred))
        self.fp.assign_add(tf.reduce_sum((1 - y_true) * y_pred))
        self.fn.assign_add(tf.reduce_sum(y_true * (1 - y_pred)))

    def result(self):
        precision = self.tp / (self.tp + self.fp + tf.keras.backend.epsilon())
        recall = self.tp / (self.tp + self.fn + tf.keras.backend.epsilon())
        return 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))

    def reset_states(self):
        self.tp.assign(0)
        self.fp.assign(0)
        self.fn.assign(0)

# 7. Model Architecture Construction
def create_cnn_model(input_shape):
    model = Sequential([
        Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(0.001), input_shape=input_shape),
        BatchNormalization(),
        MaxPooling2D(pool_size=(2, 2)),
        
        Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),
        BatchNormalization(),
        MaxPooling2D(pool_size=(2, 2)),
        
        Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),
        BatchNormalization(),
        MaxPooling2D(pool_size=(2, 2)),
        
        Conv2D(256, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),
        BatchNormalization(),
        MaxPooling2D(pool_size=(2, 2)),
        
        Conv2D(512, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),
        BatchNormalization(),
        MaxPooling2D(pool_size=(2, 2)),
        
        Flatten(),
        Dense(256, activation='relu', kernel_regularizer=l2(0.001)),
        Dropout(0.5),
        Dense(128, activation='relu', kernel_regularizer=l2(0.001)),
        Dropout(0.5),
        Dense(1, activation='sigmoid')
    ])
    return model

input_shape = (128, 128, 3)
model = create_cnn_model(input_shape)

# Optimization Setup
optimizer = AdamW(learning_rate=LEARNING_RATE, weight_decay=1e-4)
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', F1Score()])
model.summary()

# 8. Cosine Learning Rate Annealing
def cosine_scheduler(epoch, lr):
    return LEARNING_RATE * 0.5 * (1 + math.cos(math.pi * epoch / EPOCHS))

cosine_lr_scheduler = LearningRateScheduler(cosine_scheduler, verbose=1)

# 9. Training Callbacks
checkpoint_filepath = 'best_model.keras'
model_checkpoint = ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_f1_score', mode='max', save_best_only=True, verbose=1)
early_stopping = EarlyStopping(monitor='val_f1_score', mode='max', patience=10, restore_best_weights=True, verbose=1)

callbacks = [cosine_lr_scheduler, model_checkpoint, early_stopping]

# 10. Training Execution (Using 50% subset for efficient experimentation)
steps_per_epoch = math.ceil((train_generator.samples * 0.5) / BATCH_SIZE)
print(f"Total steps per epoch: {steps_per_epoch}")

history = model.fit(
    train_generator,
    steps_per_epoch=steps_per_epoch,
    epochs=EPOCHS,
    validation_data=val_generator,
    class_weight=class_weights_dict,
    callbacks=callbacks,
)

# 11. Model Evaluation
model.load_weights(checkpoint_filepath)
test_loss, test_accuracy, test_f1 = model.evaluate(test_generator, verbose=1)
print(f"Final Test Accuracy: {test_accuracy:.4f}")
print(f"Final Test F1 Score: {test_f1:.4f}")

# 12. Detailed Classification Report
y_true = test_generator.classes
y_pred_probs = model.predict(test_generator)
y_pred = (y_pred_probs > 0.5).astype(int).flatten()
print(classification_report(y_true, y_pred, target_names=list(test_generator.class_indices.keys())))

# 13. Metrics Visualization
plt.figure(figsize=(18, 5))
plt.subplot(1, 3, 1)
plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Val')
plt.title('Accuracy Evolution')
plt.legend()

plt.subplot(1, 3, 2)
plt.plot(history.history['f1_score'], label='Train')
plt.plot(history.history['val_f1_score'], label='Val')
plt.title('F1 Score Evolution')
plt.legend()

plt.subplot(1, 3, 3)
plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Val')
plt.title('Loss Reduction')
plt.legend()
plt.show()

# 14. Matrix Analysis
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_generator.class_indices.keys(), yticklabels=test_generator.class_indices.keys())
plt.title('Final Confusion Matrix')
plt.ylabel('True Class')
plt.xlabel('Predicted Class')
plt.show()  
